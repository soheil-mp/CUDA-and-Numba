{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libaries\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba import cuda, types, float32, int32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Intro\n",
    "\n",
    "---\n",
    "\n",
    "There are several programming models for programming GPUs, including CUDA, OpenCL, and DirectCompute. In this tutorial, we will focus on CUDA using numba."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown of GPU:\n",
    "- __Thread__: A single unit of computation. Think of it as a worker in a factory, performing a specific task.\n",
    "\n",
    "- __Block__: A group of threads working together. Imagine it as a team of workers in the factory, each doing their part to complete a larger job.\n",
    "\n",
    "- __Grid__: A grid is a collection of blocks. It is a higher level of organization for parallel computation. You can think of a grid as an entire factory with multiple teams (blocks) of workers (threads).\n",
    "\n",
    "A __kernel__ is a function that runs on the GPU, executed by multiple threads in parallel. It is the core piece of code that you want to accelerate using the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce GTX 1050 Ti'                              [SUPPORTED]\n",
      "                      Compute Capability: 6.1\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-babb66a5-ecc1-224b-b27f-ff53d2910c66\n",
      "                                Watchdog: Enabled\n",
      "                            Compute Mode: WDDM\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n",
      "Device name: b'NVIDIA GeForce GTX 1050 Ti'\n",
      "Compute capability: (6, 1)\n",
      "Maximum threads per block: 1024\n",
      "Maximum block dimensions: (1024, 1024, 64)\n",
      "Maximum grid dimensions: (2147483647, 65535, 65535)\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "#    CURRENT GPU INFO    #\n",
    "##########################\n",
    "\n",
    "# Detect the available GPUs\n",
    "cuda.detect()\n",
    "\n",
    "# Get the current GPU device\n",
    "device = cuda.get_current_device()\n",
    "\n",
    "# Print device properties\n",
    "print(f\"Device name: {device.name}\")\n",
    "print(f\"Compute capability: {device.compute_capability}\")\n",
    "print(f\"Maximum threads per block: {device.MAX_THREADS_PER_BLOCK}\")\n",
    "print(f\"Maximum block dimensions: ({device.MAX_BLOCK_DIM_X}, {device.MAX_BLOCK_DIM_Y}, {device.MAX_BLOCK_DIM_Z})\")\n",
    "print(f\"Maximum grid dimensions: ({device.MAX_GRID_DIM_X}, {device.MAX_GRID_DIM_Y}, {device.MAX_GRID_DIM_Z})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Device Funtion Compilation\n",
    "\n",
    "---\n",
    "\n",
    "Numba is a just-in-time (JIT) compiler for Python that allows users to accelerate code written in pure Python or NumPy with the help of graphics processing units (GPUs) or multicore CPUs.\n",
    "\n",
    "__Device functions__ are functions that are only callable from within a CUDA kernel and are not callable from the host. These functions are used to perform small computations that can be parallelized on the GPU.\n",
    "\n",
    "`@cuda.jit(device=True)` is a decorator used to define __device functions__. In other word, it can only be called from other functions running on the device (i.e. GPU).\n",
    "\n",
    "`@cuda.jit` is a decorator used to define __kernels__. In other word, it can be called from host (i.e. CPU) to perform parallel computations on device (i.e. GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the kernel function to multiply two arrays\n",
    "@cuda.jit\n",
    "def multiply_kernel(a, b, c):\n",
    "    i = cuda.grid(1)                    # Get the thread ID\n",
    "    if i < a.shape[0]:                  # Check if the thread is within the array bounds\n",
    "        c[i] = multiply(a[i], b[i])     # Call the device function to multiply two elements\n",
    "\n",
    "# Define the device function to multiply two elements\n",
    "@cuda.jit(device=True)\n",
    "def multiply(x, y):\n",
    "    return x * y                        # Perform the multiplication\n",
    "\n",
    "# Define the kernel function to add two arrays\n",
    "@cuda.jit\n",
    "def add_kernel(a, b, c):\n",
    "    i = cuda.grid(1)                     # Get the thread ID\n",
    "    if i < a.shape[0]:                   # Check if the thread is within the array bounds\n",
    "        c[i] = a[i] + b[i]               # Perform the addition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Thread, Block, and Grid Indexing\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute current __thread index__:\n",
    "- `cuda.threadIdx.x`: thread index along the x-axis\n",
    "- `cuda.threadIdx.y`: thread index along the y-axis\n",
    "- `cuda.threadIdx.z`: thread index along the z-axis\n",
    "\n",
    "Compute __global thread index__:\n",
    "- `tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x`\n",
    "\n",
    "Compute current __block index__:\n",
    "- `cuda.blockIdx.x`: block index along the x-axis\n",
    "- `cuda.blockIdx.y`: block index along the y-axis\n",
    "- `cuda.blockIdx.z`: block index along the z-axis\n",
    "\n",
    "__Grid indexing__:\n",
    "- `cuda.grid()`: total number of threads in the grid along each axis\n",
    "- `n_threads = cuda.grid(1)`: total number of threads along the x-axis.\n",
    "- `n_threads = cuda.grid(2)`: total number of threads along the y-axis\n",
    "- `n_threads = cuda.grid(3)`: total number of threads along the z-axis\n",
    "\n",
    "__Block size__ and __grid size__:\n",
    "- `cuda.blockDim`: Get the block size along each axis \n",
    "- `cuda.gridDim`: Get the grid size along each axis\n",
    "\n",
    "You cannot __print__ an statement on GPU. You must first transfer it to CPU and then print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample function for printing the current thread, block, and grid dimensions\n",
    "@cuda.jit\n",
    "def sample_kernel(output):\n",
    "    \n",
    "    # Thread coordinates (currently)\n",
    "    x, y, z = cuda.threadIdx.x, cuda.threadIdx.y, cuda.threadIdx.z\n",
    "\n",
    "    # Block coordinates (currently)\n",
    "    bx, by, bz = cuda.blockIdx.x, cuda.blockIdx.y, cuda.blockIdx.z\n",
    "\n",
    "    # Grid dimensions\n",
    "    grid_dim_x, grid_dim_y, grid_dim_z = cuda.gridDim.x, cuda.gridDim.y, cuda.gridDim.z\n",
    "\n",
    "    # Index for each thread in the grid\n",
    "    index = x + bx * cuda.blockDim.x + \\\n",
    "            (y + by * cuda.blockDim.y) * grid_dim_x * cuda.blockDim.x + \\\n",
    "            (z + bz * cuda.blockDim.z) * grid_dim_x * cuda.blockDim.x * grid_dim_y * cuda.blockDim.y\n",
    "\n",
    "    # Global thread index\n",
    "    tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "\n",
    "    # Store the information in the output array\n",
    "    output[index] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid and block dimensions\n",
    "threads_per_block = (4, 4, 1)\n",
    "blocks_per_grid = (2, 2, 1)\n",
    "\n",
    "# Calculate the total number of threads in the grid\n",
    "total_threads = threads_per_block[0] * threads_per_block[1] * threads_per_block[2] * \\\n",
    "                blocks_per_grid[0] * blocks_per_grid[1] * blocks_per_grid[2]\n",
    "\n",
    "# Create an empty array to store the output\n",
    "output = np.zeros(total_threads, dtype=np.int32)\n",
    "\n",
    "# Launch the kernel on the GPU\n",
    "sample_kernel[blocks_per_grid, threads_per_block](output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the result from the GPU to the CPU\n",
    "cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: 0\n",
      "Index 1: 1\n",
      "Index 2: 2\n",
      "Index 3: 3\n",
      "Index 4: 4\n",
      "Index 5: 5\n",
      "Index 6: 6\n",
      "Index 7: 7\n",
      "Index 8: 8\n",
      "Index 9: 9\n",
      "Index 10: 10\n",
      "Index 11: 11\n",
      "Index 12: 12\n",
      "Index 13: 13\n",
      "Index 14: 14\n",
      "Index 15: 15\n",
      "Index 16: 16\n",
      "Index 17: 17\n",
      "Index 18: 18\n",
      "Index 19: 19\n",
      "Index 20: 20\n",
      "Index 21: 21\n",
      "Index 22: 22\n",
      "Index 23: 23\n",
      "Index 24: 24\n",
      "Index 25: 25\n",
      "Index 26: 26\n",
      "Index 27: 27\n",
      "Index 28: 28\n",
      "Index 29: 29\n",
      "Index 30: 30\n",
      "Index 31: 31\n",
      "Index 32: 32\n",
      "Index 33: 33\n",
      "Index 34: 34\n",
      "Index 35: 35\n",
      "Index 36: 36\n",
      "Index 37: 37\n",
      "Index 38: 38\n",
      "Index 39: 39\n",
      "Index 40: 40\n",
      "Index 41: 41\n",
      "Index 42: 42\n",
      "Index 43: 43\n",
      "Index 44: 44\n",
      "Index 45: 45\n",
      "Index 46: 46\n",
      "Index 47: 47\n",
      "Index 48: 48\n",
      "Index 49: 49\n",
      "Index 50: 50\n",
      "Index 51: 51\n",
      "Index 52: 52\n",
      "Index 53: 53\n",
      "Index 54: 54\n",
      "Index 55: 55\n",
      "Index 56: 56\n",
      "Index 57: 57\n",
      "Index 58: 58\n",
      "Index 59: 59\n",
      "Index 60: 60\n",
      "Index 61: 61\n",
      "Index 62: 62\n",
      "Index 63: 63\n"
     ]
    }
   ],
   "source": [
    "# Print the output\n",
    "for i, index in enumerate(output):\n",
    "    print(f\"Index {i}: {index}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "__Set thead, block size when launching a kernel__:\n",
    "- When launching a kernel, use the `threadsperblock` parameter of [number of threads per block along the x-axis, number of threads per block along the y-axis, number of threads per block along the z-axis]. For example, to launch a kernel with 64 threads per block along the x-axis, we can use:\n",
    "```python\n",
    "my_kernel[64, 1, 1](...)\n",
    "```\n",
    "- When launching a kernel, use the `blockspergrid` parameter of [(number of blocks per grid along the x-axis,), (number of blocks per grid along the y-axis,), (number of blocks per grid along the z-axis,)]. For example, to launch a kernel with 16 blocks per grid along the x-axis, we can use:\n",
    "```python\n",
    "my_kernel[(16,), (64,), (64,)](...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def vector_add(a, b, c):\n",
    "    i = cuda.grid(1) # get global thread index in 1D\n",
    "    if i < a.shape[0]:\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "def main():\n",
    "    n = 1024\n",
    "    a = cuda.to_device(np.ones(n))\n",
    "    b = cuda.to_device(np.ones(n))\n",
    "    c = cuda.device_array(n)\n",
    "    threads_per_block = 128\n",
    "    blocks_per_grid = (n + threads_per_block - 1) // threads_per_block\n",
    "    vector_add[blocks_per_grid, threads_per_block](a, b, c)\n",
    "    result = c.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Memory Hierarchy\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown of memory in GPU: There are different types of memory in a GPU, each with different access speeds and scope. The main ones are:\n",
    "- __Global memory__: Accessible by all threads, but with higher latency.\n",
    "- __Shared memory__: Faster memory shared within a block, enabling cooperation between threads in the same block.\n",
    "- __Local memory__: Private memory for each thread.\n",
    "- __Constant and texture memory__: Special types of memory with caching, used for specific purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CUDA kernel\n",
    "@cuda.jit\n",
    "def memory_demo(input_array, output_array):\n",
    "\n",
    "    # Global memory\n",
    "    idx = cuda.grid(1)\n",
    "    blockDim = cuda.blockDim.x\n",
    "\n",
    "    # Allocate shared memory array dynamically\n",
    "    shared_mem = cuda.shared.array(shape=0, dtype=types.float32)\n",
    "\n",
    "    if idx < input_array.size:\n",
    "        \n",
    "        # Local memory\n",
    "        local_value = input_array[idx]\n",
    "\n",
    "        # Shared memory\n",
    "        shared_mem[cuda.threadIdx.x] = local_value\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Combine the values and store in the output array\n",
    "        output_array[idx] = local_value * shared_mem[cuda.threadIdx.x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soheil.Mohammadpour\\.conda\\envs\\predator\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Array: [  0.   1.   4.   9.  16.  25.  36.  49.  64.  81. 100. 121. 144. 169.\n",
      " 196. 225.]\n"
     ]
    }
   ],
   "source": [
    "# Create an input and output array on the host\n",
    "input_array = np.arange(16, dtype=np.float32)\n",
    "output_array = np.zeros_like(input_array)\n",
    "\n",
    "# Allocate memory on the device\n",
    "d_input_array = cuda.to_device(input_array)\n",
    "d_output_array = cuda.to_device(output_array)\n",
    "\n",
    "# Define block and grid sizes\n",
    "block_size = 4\n",
    "grid_size = (input_array.size + block_size - 1) // block_size\n",
    "\n",
    "# Call the CUDA kernel with dynamic shared memory size\n",
    "memory_demo[grid_size, block_size, 0, block_size * 4](d_input_array, d_output_array)\n",
    "\n",
    "# Copy the result from the GPU to the CPU\n",
    "d_output_array.copy_to_host(output_array)\n",
    "\n",
    "# Print the output array\n",
    "print(\"Output Array:\", output_array)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Numba Decorator\n",
    "\n",
    "---\n",
    "\n",
    "1. `@jit`: This decorator is used to mark a function for just-in-time (JIT) compilation. When this decorator is applied to a function, Numba compiles the Python code into native machine code for improved performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit\n",
    "def my_func(x, y):\n",
    "    return x + y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `@njit`: This decorator is similar to @jit, but is more strict in its type checking. It requires all types to be explicitly defined and will raise an error if any types are inferred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit(int32(int32, int32))\n",
    "def my_func(x, y):\n",
    "    return x + y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `@vectorize`: This decorator is used to create a universal function that can operate on numpy arrays of any shape and size. It is used to parallelize element-wise computations across arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize, int32\n",
    "\n",
    "@vectorize([int32(int32, int32)])\n",
    "def my_func(x, y):\n",
    "    return x + y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `@guvectorize`: This decorator is similar to @vectorize, but it allows for more complex element-wise computations that may require more than two inputs. It is used to parallelize element-wise computations across arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import guvectorize, int32\n",
    "\n",
    "# In this example, x and y are input arrays of size n, and res is an output array of size n. The function computes res[i] = x[i] + y[i] for each element in the arrays.\n",
    "@guvectorize([(int32[:], int32[:], int32[:])], '(n), (n) -> (n)')\n",
    "def my_func(x, y, res):\n",
    "    for i in range(x.shape[0]):\n",
    "        res[i] = x[i] + y[i]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Synchronization \n",
    "\n",
    "---\n",
    "\n",
    "In this section, you'll use the tools for controlling the order and timing of the operations in parallel computing.\n",
    "\n",
    "__Synchronization__ is the coordination of multiple threads or processes to ensure that they execute in a specific order or at a specific time. In parallel computing, synchronization is important to prevent race conditions and to ensure that operations are performed correctly.\n",
    "\n",
    "Synchronization is achieved using synchronization primitives such as barriers and events:\n",
    "1. __Barriers__: A barrier is a synchronization primitive that forces threads to wait until all other threads in the group have reached the barrier before allowing any thread to proceed.  The `--syncthreads()` function is used to synchronize threads within a block. All threads in the block must reach the `--syncthreads()` call before any thread can proceed beyond it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this kernel, each thread increments the element a[i] and then waits for all other threads in the block to complete this operation using the __syncthreads() function. After the barrier, each thread multiplies the element a[i] by 2.\n",
    "@cuda.jit\n",
    "def kernel(a):\n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    bdim = cuda.blockDim.x\n",
    "    i = tid + bid * bdim\n",
    "    a[i] += 1\n",
    "    cuda.syncthreads()\n",
    "    a[i] *= 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Events__: An event is a synchronization primitive that allows threads to synchronize their execution across multiple blocks. Events are implemented using the cudaEvent_t data type and a set of functions to create, record, and wait for events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soheil.Mohammadpour\\.conda\\envs\\predator\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 64 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\Soheil.Mohammadpour\\.conda\\envs\\predator\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:885: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\Soheil.Mohammadpour\\.conda\\envs\\predator\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 64 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\Soheil.Mohammadpour\\.conda\\envs\\predator\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:885: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def kernel1(a):\n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    bdim = cuda.blockDim.x\n",
    "    i = tid + bid * bdim\n",
    "    a[i] += 1\n",
    "\n",
    "@cuda.jit\n",
    "def kernel2(a):\n",
    "    tid = cuda.threadIdx.x\n",
    "    bid = cuda.blockIdx.x\n",
    "    bdim = cuda.blockDim.x\n",
    "    i = tid + bid * bdim\n",
    "    a[i] *= 2\n",
    "\n",
    "a = np.ones(1024).astype(np.float32)\n",
    "stream = cuda.stream()\n",
    "event1 = cuda.event()\n",
    "event2 = cuda.event()\n",
    "\n",
    "kernel1[64, 16, stream](a)\n",
    "event1.record(stream)\n",
    "kernel2[64, 16, stream](a)\n",
    "event2.record(stream)\n",
    "event1.synchronize()\n",
    "event2.synchronize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Memory Management \n",
    "\n",
    "---\n",
    "\n",
    "__Memory management__ is a crucial aspect of programming for GPUs, as it directly affects the performance of the application, and it can be used to optimize your codes. \n",
    "\n",
    "Numba provides several functions for managing memory on the GPU\n",
    "\n",
    "1. `cuda.to_device()`:  It's used to copy a NumPy array to the device memory. It takes a NumPy array as input and returns a DeviceNDArray (i.e. array on the device). The device array can be used in CUDA kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numba.cuda.cudadrv.devicearray.DeviceNDArray at 0x21f9e8f0850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a NumPy array\n",
    "a = np.array([1, 2, 3])\n",
    "\n",
    "# Copy the array to device memory\n",
    "d_a = cuda.to_device(a)\n",
    "d_a\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `cuda.device_array()`:  It's used to allocate a device memory buffer of a specified size. It takes the size of the buffer in bytes as input and returns a DeviceNDArray object that represents the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numba.cuda.cudadrv.devicearray.DeviceNDArray at 0x21faf316500>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Allocate device memory buffer\n",
    "d_a = cuda.device_array(10, dtype=np.float32)\n",
    "d_a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `cuda.device_array_like()`: It's similar to the previous function but instead of specifying the size of the buffer, it takes a NumPy array as input and allocates a device memory buffer of the same size and data type as the input array. It returns a DeviceNDArray object that represents the allocated buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numba.cuda.cudadrv.devicearray.DeviceNDArray at 0x21faf3154e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a NumPy array\n",
    "a = np.array([1, 2, 3], dtype=np.float32)\n",
    "\n",
    "# Allocate a device memory buffer of the same size and data type as a\n",
    "d_a = cuda.device_array_like(a)\n",
    "d_a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `cuda.pinned_array()`: It's used to allocate pinned host memory, which can be accessed faster by the GPU. It takes the size of the pinned memory buffer in bytes as input and returns a NumPy array that represents the pinned memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Allocate pinned host memory\n",
    "pinned_a = cuda.pinned_array(10, dtype=np.float32)\n",
    "pinned_a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Data Transfer; Host-to-device and device-to-host\n",
    "\n",
    "---\n",
    "\n",
    "Numba provides easy-to-use memory transfer functions for moving data between the CPU and GPU memory. \n",
    "\n",
    "1. Host-to-Device Data Transfer: The `cuda.to_device()` function is used to transfer data from the host (CPU) to the device (GPU) memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numba.cuda.cudadrv.devicearray.DeviceNDArray at 0x21f9e9b91e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a NumPy array\n",
    "a = np.array([1, 2, 3, 4])\n",
    "\n",
    "# Transfer the array to the GPU memory\n",
    "d_a = cuda.to_device(a)\n",
    "d_a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Device-to-Host Data Transfer: The `copy_to_host()` function is used to transfer data from the device (GPU) memory to the host (CPU) memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a device array\n",
    "d_a = cuda.device_array(4)\n",
    "\n",
    "# Transfer the array to the CPU memory\n",
    "a = d_a.copy_to_host()\n",
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of topics you should learn to get started with Numba and CUDA:\n",
    "\n",
    "Python and NumPy: Familiarize yourself with Python and NumPy as they are the foundation for using Numba.\n",
    "\n",
    "Numba basics:\n",
    "\n",
    "Just-In-Time (JIT) compilation\n",
    "Numba decorators: @jit, @njit, @vectorize, @guvectorize\n",
    "Supported Python and NumPy features in Numba\n",
    "CUDA basics:\n",
    "\n",
    "GPU architecture and programming model\n",
    "CUDA memory hierarchy: global, shared, local, and constant memory\n",
    "CUDA threads, thread blocks, and grids\n",
    "Synchronization primitives: barriers and events\n",
    "Numba for CUDA:\n",
    "\n",
    "Numba CUDA decorators: @cuda.jit, @cuda.vectorize, @cuda.reduce\n",
    "Memory management with Numba: cuda.to_device(), cuda.device_array(), cuda.device_array_like(), cuda.pinned_array()\n",
    "Host-to-device and device-to-host data transfers: copy_to_device(), copy_to_host()\n",
    "Device function compilation with @cuda.jit(device=True)\n",
    "Thread, block, and grid indexing: cuda.grid(), cuda.gridsize(), cuda.threadIdx, cuda.blockIdx, cuda.blockDim, cuda.gridDim\n",
    "Synchronization: cuda.syncthreads()\n",
    "Atomic operations: cuda.atomic.add(), cuda.atomic.max(), etc.\n",
    "Dynamic shared memory allocation\n",
    "Performance optimization:\n",
    "\n",
    "Coalesced memory access patterns\n",
    "Shared memory utilization\n",
    "Occupancy maximization\n",
    "Thread divergence and warp execution\n",
    "Profiling tools: Nsight Compute, Nsight Systems, and nvprof\n",
    "Performance optimization strategies: loop unrolling, constant memory utilization, etc.\n",
    "Interoperability:\n",
    "\n",
    "Interfacing Numba with other GPU libraries like CuPy or PyTorch\n",
    "Interfacing Numba with C/C++ code using the Python C-API, ctypes, or CFFI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
